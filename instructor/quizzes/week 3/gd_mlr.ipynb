{
 "cells": [
  {
   "cell_type": "raw",
   "id": "310cc7dd-491a-44f7-8851-0a3861c61e8f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 2: Multiple Linear Regression with Gradient Descent\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    number-sections: true\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92485c6b-75a4-4e2a-8794-84338e0db8bc",
   "metadata": {},
   "source": [
    "\n",
    " Now you're ready to tackle the real world, where house prices depend on more than just square footage. In this lab, you'll build a **multi-feature prediction model** that considers size, bedrooms, floors, and age simultaneously!\n",
    "\n",
    "## üè† Real Estate Challenge: The Complete Picture\n",
    "\n",
    "While single-variable regression was a great start, real estate pricing is much more complex:\n",
    "\n",
    "- **Size matters** - but so does the number of **bedrooms**\n",
    "- **Multiple floors** can increase value significantly  \n",
    "- **Age of the house** often affects market price\n",
    "- **All features interact** to determine the final price\n",
    "\n",
    "Your mission: Build a sophisticated model that captures these complex relationships!\n",
    "\n",
    "##  What You'll Master in This Lab\n",
    "\n",
    "1. **Multiple Linear Regression** - Handle multiple input features simultaneously\n",
    "2. **Vector Operations in NumPy** - Leverage efficient mathematical computations\n",
    "3. **Feature Scaling Techniques** - Ensure optimal algorithm performance\n",
    "4. **Gradient Descent for Multiple Variables** - Extend your optimization skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864c8464-0e4d-4094-8c3a-94c63feed394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118779ce-9bce-4a74-9017-02bd84ddb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = np.loadtxt(\"Datasets/houses.txt\", delimiter=',')\n",
    "X_train, y_train = data[:,:-1], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea1c2cf-5411-446d-ad6c-3f4fe97a207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (100, 4), X Type:<class 'numpy.ndarray'>)\n",
      "y Shape: (100,), y Type:<class 'numpy.ndarray'>)\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b315340-f5fc-4406-b634-7ce8054c91fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34961b-cd4b-46e9-9842-faf47ba20a8c",
   "metadata": {},
   "source": [
    "##  MLR: Single Prediction\n",
    "\n",
    "For a single prediction, you can use loop vs vector way\n",
    "\n",
    "**Loop Notation**\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "\n",
    "**Vector Notation (The Elegant Way):**\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b \\quad (2)\n",
    "$$\n",
    "\n",
    "where $\\cdot$ is a vector **dot product**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfa9f0-0aa6-47ec-b810-674ceff9bb42",
   "metadata": {},
   "source": [
    "###  Single Prediction, loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75700b9-0e9e-491f-8da3-836705922afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) example with multiple features\n",
    "      w (ndarray): Shape (m,) model parameters, each feature has a weight/parameter    \n",
    "      b (scalar):  model bias parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(m):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305051d7-a634-49dc-997b-b57f234f2f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [952.   2.   1.  65.]\n",
      "f_wb shape (), prediction: -575.5059514105919\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e911fe-22b1-4e81-8413-ad063a6e131f",
   "metadata": {},
   "source": [
    "Note the shape of `x_vec`. It is a 1-D NumPy vector with 4 elements, (4,). The result, `f_wb` is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0a913-c24e-409d-86c7-8587521bb24a",
   "metadata": {},
   "source": [
    "###  Single Prediction, vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613f470-db91-48d2-9126-aa8f96754375",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to simplify predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f211bf-aa75-457b-bc20-2d422a2eb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) example with multiple features\n",
    "      w (ndarray): Shape (m,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e2331d-1cbc-4bf7-ba97-a9350ceb358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [952.   2.   1.  65.]\n",
      "f_wb shape (), prediction: -575.5059514105919\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b24cb5-156a-46ae-972a-b174e758d795",
   "metadata": {},
   "source": [
    "Both approaches produce identical results, but understanding both will make you a stronger programmer!\n",
    "\n",
    "### Why Learn Both Methods? \n",
    "\n",
    "- **Loop Method**: Builds mathematical intuition  \n",
    "- **Vector Method**: Professional-grade efficiency\n",
    "- **Foundation**: Prepares you for DL\n",
    "\n",
    "##  Understanding `np.dot` - The Heart of DL\n",
    "\n",
    "`np.dot` is fundamental in machine learning - it's the building block of neural networks and matrix operations.\n",
    "\n",
    "### üßÆ What is a Dot Product?\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190413155221/dotproduct.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "**Simple definition:** Multiply corresponding elements, then sum them up.\n",
    "\n",
    "$$\\text{dot product} = \\sum_{i=0}^{n-1} a_i \\times b_i$$\n",
    "\n",
    "**Example:** `[1,2,3] ¬∑ [4,5,6] = (1√ó4) + (2√ó5) + (3√ó6) = 4 + 10 + 18 = 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c54d154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "# test 1-D\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2211356",
   "metadata": {},
   "source": [
    "###  Speed Test: Loops vs Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a59c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dot(a, b): \n",
    "    \"\"\"\n",
    "   Compute the dot product of two vectors\n",
    " \n",
    "    Args:\n",
    "      a (ndarray (n,)):  input vector \n",
    "      b (ndarray (n,)):  input vector with same dimension as a\n",
    "    \n",
    "    Returns:\n",
    "      x (scalar): \n",
    "    \"\"\"\n",
    "    x=0\n",
    "    for i in range(a.shape[0]):\n",
    "        x = x + a[i] * b[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ede9730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b) =  2501072.5817\n",
      "Vectorized version duration: 0.0000 ms \n",
      "my_dot(a, b) =  2501072.5817\n",
      "loop version duration: 1465.5447 ms \n",
      "my_dot(a, b) =  2501072.5817\n",
      "loop version duration: 1465.5447 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)  # very large arrays\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"np.dot(a, b) =  {c:.4f}\")\n",
    "print(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = my_dot(a,b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"my_dot(a, b) =  {c:.4f}\")\n",
    "print(f\"loop version duration: {1000*(toc-tic):.4f} ms \")\n",
    "\n",
    "del(a);del(b)  #remove these big arrays from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72428e",
   "metadata": {},
   "source": [
    "\n",
    "**The performance gap isn't just impressive - it's production-critical!**\n",
    "\n",
    "###  The Technology Behind the Speed\n",
    "\n",
    "- **SIMD Processing**: Your CPU executes operations on multiple numbers simultaneously\n",
    "- **Optimized Libraries**: NumPy uses battle-tested C/Fortran libraries (BLAS, LAPACK)\n",
    "- **Memory Efficiency**: Better cache usage and reduced overhead\n",
    "\n",
    "###  Real-World Impact\n",
    "\n",
    "- **Training Speed**: 10x-100x faster on large datasets  \n",
    "- **Production Systems**: Faster inference = better user experience\n",
    "- **Scalability**: Essential for deep learning and big data applications\n",
    "\n",
    "**Bottom Line:** Loops teach the math, vectorization makes it production-ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81dea2-f84d-497f-ac18-f5fbddd466a3",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Linear Regression\n",
    "You will implement gradient descent algorithm for multiple features. As in the last lab, you will need three functions.\n",
    "\n",
    "- `compute_cost`\n",
    "- `compute_gradient`\n",
    "- `gradient_descent`\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(w, b)$ With Respect To $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad29424-8255-4e6c-b0e6-68564264f2a7",
   "metadata": {},
   "source": [
    "### Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i = 0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)^2 \\quad (3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\quad (4)\n",
    "$$\n",
    "\n",
    "In contrast to the last lab, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars, supporting multiple features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5cb57-a741-4af2-b992-01f73520f1eb",
   "metadata": {},
   "source": [
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eea2b4-3a6c-4abd-8cae-ec3f3d02bab8",
   "metadata": {},
   "source": [
    "#### Task 1: Implement the `compute_cost` below to compute the cost $J(w,b)$.\n",
    "\n",
    "Note that you need to use the dot product for single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc03de-c280-45f5-8b12-69c9d6837206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for multiple linear regression using Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model weight parameters  \n",
    "      b (scalar)       : model bias parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): The cost J(w,b) representing how well the parameters\n",
    "                     fit the training data. Lower cost = better fit.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Get the number of training examples\n",
    "    m = # YOUR CODE\n",
    "    \n",
    "    # Step 2: Initialize cost accumulator\n",
    "    cost = 0.0\n",
    "    \n",
    "    # Step 3: Loop through each training example\n",
    "    for i in # YOUR CODE:  iterate through all examples:                                \n",
    "        \n",
    "        # Step 4: Calculate prediction for example i using vectorized dot product\n",
    "        f_wb_i = # YOUR CODE\n",
    "        \n",
    "        # Step 5: Calculate squared error and add to cost\n",
    "        cost = # YOUR CODE\n",
    "    \n",
    "    # Step 6: Calculate final MSE cost (divide by 2m)\n",
    "    cost = # YOUR CODE\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3e8f-d533-46c9-ba86-cbdcde46aa92",
   "metadata": {},
   "source": [
    "#### Test your implementation and take the quiz on canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744ddc-ad83-4cb2-9f92-cbe2611c09ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61792238-4338-42ad-974d-5f133f4199aa",
   "metadata": {},
   "source": [
    "### Implement Gradient Descent With Multiple Variable Using Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef577a8-64c0-4897-afd3-f407afd0c662",
   "metadata": {},
   "source": [
    "\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples on the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction for the $i$ th example, while $y^{(i)}$ is the corresponding target value\n",
    "\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f221d0-75ab-46a8-bdce-8e42005183a5",
   "metadata": {},
   "source": [
    "#### Task 2: Implement the `compute_gradient` function with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. \n",
    "\n",
    "In this version, you will be asked to use for loops, specifically:\n",
    "\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1783bf-5c9e-4f44-9805-f4676aea57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for multiple linear regression using nested loops.\n",
    "    \n",
    "    The gradient tells us which direction to adjust our parameters (w, b) to reduce cost.\n",
    "    For multiple features, we need gradients for each weight parameter and the bias.\n",
    "    \n",
    "    Mathematical foundations:\n",
    "    - dJ/dw_j = (1/m) * Œ£(error * x_j) for each feature j\n",
    "    - dJ/db   = (1/m) * Œ£(error) across all examples\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Get dimensions - number of examples (m) and features (n)\n",
    "    m, n = # YOUR CODE\n",
    "    \n",
    "    # Step 2: Initialize gradient accumulators\n",
    "    dj_dw = np.zeros((n,))  # Array to store gradients for each weight\n",
    "    dj_db = 0.              # Scalar to store gradient for bias\n",
    "    \n",
    "    # Step 3: Outer loop - iterate through all training examples\n",
    "    for i in # YOUR CODE: loop through examples:\n",
    "        \n",
    "        # Step 4: Calculate prediction error for current example\n",
    "        f_wb_i = # YOUR CODE\n",
    "        err = # YOUR CODE\n",
    "        \n",
    "        # Step 5: Inner loop - calculate gradient contribution for each feature\n",
    "        for j in # YOUR CODE: loop through features\n",
    "            # Gradient for weight j\n",
    "            dj_dw[j] = # YOUR CODE\n",
    "        \n",
    "        # Step 6: Accumulate gradient for bias (no feature multiplication needed)\n",
    "        dj_db = # YOUR CODE\n",
    "    \n",
    "    # Step 7: Average the accumulated gradients over all examples\n",
    "    dj_dw = # YOUR CODE\n",
    "    dj_db = # YOUR CODE\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958da6f-da3d-444f-bdfa-ae0426f3a73c",
   "metadata": {},
   "source": [
    "#### Test your implementation\n",
    "\n",
    "You can check if your implementation was correct by running the following test code: Please fill in the blanks in the Canvas quiz accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8b89f-17ab-49e6-af6c-1672ab8de70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78c2ac-c11e-4edd-ac27-607ae3db2d15",
   "metadata": {},
   "source": [
    "#### Task 3: Implement the `gradient_descent` function with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb9223-0b9f-48b2-afb0-13b5cfddb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to optimize parameters w and b for multiple linear regression.\n",
    "    \n",
    "    This is the main optimization algorithm that iteratively improves our model parameters\n",
    "    by moving them in the direction that reduces cost. Each iteration:\n",
    "    1. Computes gradients (directions to move parameters)\n",
    "    2. Updates parameters using the gradient descent rule\n",
    "    3. Tracks progress for analysis\n",
    "    \n",
    "    Mathematical foundation:\n",
    "    w := w - Œ± * (‚àÇJ/‚àÇw)    [Update weights]\n",
    "    b := b - Œ± * (‚àÇJ/‚àÇb)    [Update bias]\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters (starting weights)  \n",
    "      b_in (scalar)       : initial model parameter (starting bias)\n",
    "      cost_function       : function to compute cost J(w,b)\n",
    "      gradient_function   : function to compute gradients ‚àÇJ/‚àÇw, ‚àÇJ/‚àÇb\n",
    "      alpha (float)       : Learning rate (step size for parameter updates)\n",
    "      num_iters (int)     : number of iterations to run optimization\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Optimized weight parameters \n",
    "      b (scalar)       : Optimized bias parameter\n",
    "      J_history (list): Cost at each iteration (for plotting learning curves)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize tracking arrays for visualization and analysis\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # Create copy to avoid modifying input parameters\n",
    "    b = b_in\n",
    "    \n",
    "    # Main optimization loop - iterate for specified number of steps\n",
    "    for i in # YOUR CODE: range(num_iters) to iterate through all optimization steps:\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Step 1: Calculate gradients using your gradient function\n",
    "        # This tells us which direction and how much to adjust each parameter\n",
    "        dj_db, dj_dw = # YOUR CODE\n",
    "\n",
    "        # Step 2: Update parameters using gradient descent rule\n",
    "        # Move parameters in opposite direction of gradient (descent)\n",
    "        w = # YOUR CODE\n",
    "        b = # YOUR CODE\n",
    "      \n",
    "        # Step 3: Track progress by computing and storing current cost\n",
    "        if i < 100000:  # Prevent memory issues for very long training runs\n",
    "            current_cost = # YOUR CODE\n",
    "            J_history.append(current_cost)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Display progress periodically (every 10% of total iterations)\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history  # Return optimized parameters and training history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef25350-b20c-436d-aef6-68fb90091c5c",
   "metadata": {},
   "source": [
    "#### Test Your Gradient Descent Implementation\n",
    "\n",
    "**üß™ Validation Phase: Put Your Algorithm to the Test!**\n",
    "\n",
    "Now it's time to see your gradient descent algorithm in action! The cell below will:\n",
    "\n",
    "**What the test does:**\n",
    "\n",
    "- **Initializes parameters** to zero (clean slate approach)\n",
    "- **Runs 10,000 iterations** of gradient descent optimization  \n",
    "- **Uses learning rate Œ± = 5.0e-7** (carefully chosen for stability)\n",
    "- **Tracks convergence** through the cost history\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "- **Final cost** should be significantly lower than initial cost\n",
    "- **Parameters (w, b)** should converge to reasonable values\n",
    "- **Learning curve** should show steady cost reduction\n",
    "\n",
    "**üìã Canvas Quiz Instructions:**\n",
    "\n",
    "1. **Run the cell below** and observe the final parameter values\n",
    "2. **Note the final cost** after 10,000 iterations\n",
    "3. **Record the optimized weights** `w_final` and bias `b_final`\n",
    "4. **Answer the corresponding questions** on Canvas with these values\n",
    "\n",
    "**üí° Success Indicators:**\n",
    "\n",
    "- Cost decreases consistently over iterations\n",
    "- Parameters stabilize to meaningful values\n",
    "- No warning messages or convergence issues\n",
    "\n",
    "Ready to see your implementation work? Run the test below! ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38407f5a-866a-48b6-944b-20a9e06cc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f}, {w_final} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e617e2",
   "metadata": {},
   "source": [
    "#### üìà Analyzing the Learning Curve: Understanding Convergence Behavior\n",
    "\n",
    "**Why Learning Curves Matter:**\n",
    "\n",
    "Learning curves are your **diagnostic tool** for understanding how well gradient descent is performing.\n",
    "\n",
    "**Two-View Analysis:**\n",
    "\n",
    "- **Left Plot**: Complete training history (full perspective)\n",
    "- **Right Plot**: Tail end focus (detailed convergence behavior)\n",
    "\n",
    "Let's visualize your algorithm's learning journey! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd611c5-fbc9-42ed-b66f-bc9b2207080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the learning curve to analyze gradient descent performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "\n",
    "# Left plot: Full training history\n",
    "ax1.plot(J_hist, 'b-', linewidth=2, alpha=0.8)\n",
    "ax1.set_title(\" Complete Learning Curve\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Cost J(w,b)', fontsize=10)\n",
    "ax1.set_xlabel('Iteration Step', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Focus on tail end (last 60% of training)\n",
    "start = 4000\n",
    "tail_iterations = start + np.arange(len(J_hist[start:]))\n",
    "ax2.plot(tail_iterations, J_hist[start:], 'r-', linewidth=2, alpha=0.8)\n",
    "ax2.set_title(\" Convergence Detail (Final Phase)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Cost J(w,b)', fontsize=10)\n",
    "ax2.set_xlabel('Iteration Step', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add informative annotations\n",
    "if len(J_hist) > 0:\n",
    "    initial_cost = J_hist[0]\n",
    "    final_cost = J_hist[-1]\n",
    "    cost_reduction = ((initial_cost - final_cost) / initial_cost) * 100\n",
    "    \n",
    "    # Add cost reduction info to the plots\n",
    "    ax1.text(0.02, 0.98, f'Initial Cost: {initial_cost:.2e}', \n",
    "             transform=ax1.transAxes, fontsize=9, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    ax2.text(0.02, 0.98, f'Final Cost: {final_cost:.2e}\\nReduction: {cost_reduction:.1f}%', \n",
    "             transform=ax2.transAxes, fontsize=9, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.suptitle(' Gradient Descent Learning Curve Analysis', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# üìã Performance Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" LEARNING CURVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\" Initial Cost:      {J_hist[0]:,.2e}\")\n",
    "print(f\" Final Cost:       {J_hist[-1]:,.2e}\")\n",
    "print(f\" Cost Reduction:   {((J_hist[0] - J_hist[-1]) / J_hist[0]) * 100:.2f}%\")\n",
    "print(f\" Total Iterations: {len(J_hist):,}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b876d-3f98-4ee8-af18-558a2636e932",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. Let's next explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9980fe-495b-4cb4-a7e3-b39c44bf5dfd",
   "metadata": {},
   "source": [
    "## Feature Scaling in Gradient Descent\n",
    "\n",
    "Let's view the dataset and its features by plotting each feature versus price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e310d7b-0ad5-4e6f-8546-d104631c01f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e543682-f4cd-41b2-b22a-41c6e6ad07c6",
   "metadata": {},
   "source": [
    "As illustrated above, the features exhibit varying scales, a factor that holds significant importance in the gradient descent optimization algorithms, particularly when features span disparate orders of magnitude. Effective feature scaling is critical to ensure optimal convergence and performance. Two commonly used ways for feature scaling include: \n",
    " [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) (also called z-score normalization or standardization) and [MinMax Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7d8be-c979-4822-bdfc-85288cd5e8fe",
   "metadata": {},
   "source": [
    "\n",
    "### z-score normalization \n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\quad (9) \\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} \\big(x^{(i)}_j - \\mu_j\\big)^2 \\quad (10)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "To implement z-score normalization, adjust your input values as shown in this formula:\n",
    "\n",
    "$$\n",
    "x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\quad (8)\n",
    "$$\n",
    "\n",
    "where $j$ indexes a feature (a column in the matrix $\\mathbf{X}$),  \n",
    "$\\mu_j$ is the mean of all the values for feature $j$, and  \n",
    "$\\sigma_j$ is the standard deviation of feature $j$.\n",
    "\n",
    "\n",
    ">**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before. Given a new x value (living room area and number of bed-\n",
    "rooms), we must first normalize x using the mean and standard deviation\n",
    "that we had previously computed from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed9b9f-251d-4e9d-909a-3fac1112ec4f",
   "metadata": {},
   "source": [
    "#### Task 4: Implementation the `zscore_normalize_features` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6393d-90b1-42f0-a633-0a13e58c0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Computes z-score normalization (standardization) of features by column.\n",
    "    \n",
    "    Z-score normalization transforms each feature to have:\n",
    "    - Mean (Œº) = 0 \n",
    "    - Standard deviation (œÉ) = 1\n",
    "    \n",
    "    Mathematical Formula: x_norm = (x - Œº) / œÉ\n",
    "    \n",
    "    This is crucial for gradient descent because:\n",
    "    - Features on different scales can cause slow/poor convergence\n",
    "    - Normalized features allow higher learning rates\n",
    "    - Creates a more spherical cost function landscape\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Input data matrix with m examples and n features\n",
    "                         Each column represents one feature across all examples\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): Feature-normalized data (same shape as input)\n",
    "                              Each feature now has mean‚âà0, std‚âà1  \n",
    "      mu (ndarray (n,))     : Mean of each original feature (for future use)\n",
    "      sigma (ndarray (n,))  : Standard deviation of each original feature\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Calculate mean of each feature (column-wise operation)\n",
    "    # Hint: Use np.mean() with appropriate axis parameter\n",
    "    mu = # YOUR CODE - compute mean along the correct axis\n",
    "    \n",
    "    # Step 2: Calculate standard deviation of each feature (column-wise)\n",
    "    # Hint: Use np.std() with appropriate axis parameter  \n",
    "    sigma = # YOUR CODE - compute std deviation along the correct axis\n",
    "    \n",
    "    # Step 3: Apply z-score normalization formula: (X - Œº) / œÉ\n",
    "    # This transforms each feature to have mean=0, std=1\n",
    "    # Hint: Broadcasting will automatically handle the element-wise operations\n",
    "    X_norm = # YOUR CODE - apply normalization formula\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return (X_norm, mu, sigma)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb317e7-28da-47da-b1e9-94059e6e7c44",
   "metadata": {},
   "source": [
    "Let's use your implementation to normalize the data and compare it to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7be4e-9931-41c4-b5e1-465ba28aaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c09e0-d591-4380-a337-e85534088d89",
   "metadata": {},
   "source": [
    "#### Task 5: use the `StandardScaler` in Sklearn to check your implementation above and see whether you can get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93830020-d84b-4321-b3b3-2d67ce4cdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please code below, ensure that the variable names match the printed information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c67b0-f0cd-4542-9802-85350a213717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_mu_sklearn = {X_mu_sklearn}, \\nX_sigma_sklearn = {X_sigma_sklearn}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0ed40-93b1-440a-a0c0-c1af7e80f581",
   "metadata": {},
   "source": [
    "After feature scaling with z-score normalization, let's visualize the features again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b481a7-3d58-4974-965e-a60e436152e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b439e3e-e745-4b0a-82d1-f3f978a9fd0e",
   "metadata": {},
   "source": [
    "The plot above shows the relationship between two of the training set parameters, \"age\" and \"size(sqft)\". *These are plotted with equal scale*. \n",
    "\n",
    "- Left: Unnormalized: The range of values or the variance of the 'size(sqft)' feature is much larger than that of age\n",
    "- Middle: The first step removes the mean or average value from each feature. This leaves features that are centered around zero. It's difficult to see the difference for the 'age' feature, but 'size(sqft)' is clearly around zero.\n",
    "- Right: The second step divides by the standard deviation. This leaves both features centered at zero with a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1209b68-03d6-4b0e-9ce7-9048419a402d",
   "metadata": {},
   "source": [
    "After feature scaling, the peak to peak range of each column is reduced from a factor of thousands to a factor of 2-3 by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3d96b-e01b-4a44-b1bc-1565a8e16033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def norm_plot(ax, data):\n",
    "    scale = (np.max(data) - np.min(data))*0.2\n",
    "    x = np.linspace(np.min(data)-scale,np.max(data)+scale,50)\n",
    "    _,bins, _ = ax.hist(data, x, color=\"xkcd:azure\")\n",
    "    #ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    mu = np.mean(data); \n",
    "    std = np.std(data); \n",
    "    dist = norm.pdf(bins, loc=mu, scale = std)\n",
    "    \n",
    "    axr = ax.twinx()\n",
    "    axr.plot(bins,dist, color = \"orangered\", lw=2)\n",
    "    axr.set_ylim(bottom=0)\n",
    "    axr.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be69a1-b567-4518-8ebf-69c18126ed92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_train[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\");\n",
    "fig.suptitle(\"distribution of features before normalization\")\n",
    "plt.show()\n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_norm[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\"); \n",
    "fig.suptitle(\"distribution of features after normalization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f16424-2a60-4173-9852-ec5b15e09345",
   "metadata": {},
   "source": [
    "#### Task 6:  Testing Gradient Descent with Normalized Data\n",
    "\n",
    "Now for the **exciting part** - let's see how feature scaling transforms our gradient descent performance! \n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "- **Faster and better convergence** - The algorithm should reach optimal values much quicker.\n",
    "\n",
    "**Why this works:** When all features are on the same scale, gradient descent can take more direct paths toward the minimum, rather than zigzagging due to different feature magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b11fbd-866a-4dd8-888c-a44aa7e1aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 1.0e-3\n",
    "# run gradient descent \n",
    "w_norm_final, b_norm_final, J_norm_hist = gradient_descent(X_norm, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_norm_final:0.2f},{w_norm_final} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab25be-a0ff-4628-87dd-190236e5a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_norm_hist)\n",
    "start=4000\n",
    "ax2.plot(start + np.arange(len(J_norm_hist[start:])), J_norm_hist[start:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64cf6071-073c-4a04-993d-091d4cc8bf81",
   "metadata": {},
   "source": [
    "As evident from the above learning curve, the reduction in training cost accelerated notably after feature scaling. This enhancement underscores the pivotal role of feature scaling in expediting the convergence of gradient descent algorithms. Without proper scaling, these algorithms often endure oscillations, significantly delaying their journey towards the minimum point. Thus, feature scaling proves indispensable, particularly when dealing with features of varying orders of magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95cc63",
   "metadata": {},
   "source": [
    "##  Bonus Exploration: Learning Rate Sensitivity Analysis\n",
    "\n",
    "**Experiment Goal:** Investigate how different learning rates affect gradient descent performance with and without feature normalization.\n",
    "\n",
    "###  What You'll Discover:\n",
    "\n",
    "**Research Questions:**\n",
    "\n",
    "1. How does learning rate choice impact convergence with unnormalized data?\n",
    "2. Why can normalized data tolerate much larger learning rates?\n",
    "3. What happens when learning rates are too high or too low?\n",
    "\n",
    "*Try plotting the cost curves side-by-side to visualize the dramatic difference! and share your insights below*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938e661",
   "metadata": {},
   "source": [
    "##  Chapter Summary: Gradient Descent for Multiple Linear Regression & Feature Scaling\n",
    "\n",
    "**Congratulations!** You‚Äôve taken your understanding of multiple linear regression a step further by learning how to train it using gradient descent. Along the way, you also discovered how **feature scaling** can dramatically improve the efficiency of optimization. Let‚Äôs consolidate what you‚Äôve learned:\n",
    "\n",
    "###  Key Achievements in This Lab\n",
    "\n",
    "**1. Gradient Descent for Multiple Linear Regression**\n",
    "\n",
    "- ‚úÖ Extended simple linear regression to handle multiple features simultaneously\n",
    "- ‚úÖ Mastered the mathematical foundations: $f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "- ‚úÖ Implemented **single instance prediction** using vectorized `np.dot()` operations\n",
    "- ‚úÖ Built gradient descent for multiple variables using **nested for loops**\n",
    "\n",
    "\n",
    "**2. Feature Scaling Breakthrough** \n",
    "\n",
    "- ‚úÖ Discovered why feature scaling is **critical** for gradient descent optimization\n",
    "- ‚úÖ Implemented z-score normalization from scratch: $x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$\n",
    "- ‚úÖ Witnessed **dramatic performance improvements** \n",
    "- ‚úÖ Understood the optimization landscape transformation\n",
    "\n",
    "### üîÑ What We Built vs. What's Coming Next\n",
    "\n",
    "**This Lab - Loop-Based Implementation:**\n",
    "\n",
    "- Single prediction: **Vectorized** with `np.dot()`\n",
    "- Cost computation: **For loop** over all training examples\n",
    "- Gradient computation: **Nested for loops** over examples and features\n",
    "- Perfect for **learning the mathematics** step-by-step\n",
    "\n",
    "**Next Lab - Fully Vectorized Implementation:**\n",
    "\n",
    "- **All operations** will be vectorized using NumPy matrix operations\n",
    "- Eliminate **all for loops** for maximum computational efficiency\n",
    "- Process **entire datasets** in single matrix operations\n",
    "- Production-ready, **scalable** implementations\n",
    "\n",
    "###  Why This Progressive Approach Matters\n",
    "\n",
    "\n",
    "**Performance Evolution:**\n",
    "\n",
    "- **Current**: Single prediction vectorized, batch processing with loops\n",
    "- **Next**: Complete vectorization for production-scale performance\n",
    "- **Impact**: From educational clarity to industrial-strength efficiency\n",
    "\n",
    "\n",
    "###  What Awaits in the Next Lab\n",
    "\n",
    "**Fully Vectorized Gradient Descent:**\n",
    "\n",
    "- Matrix operations replacing all loops\n",
    "- Simultaneous processing of entire datasets\n",
    "- Production-level computational efficiency\n",
    "- Foundation for DL techniques\n",
    "\n",
    "**Bottom Line:** You've built rock-solid mathematical foundations using a clarity-first approach. Next, we'll transform this understanding into lightning-fast, production-ready implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ada253-e6f5-471b-bbff-7d2914e0891d",
   "metadata": {},
   "source": [
    "##  Reference\n",
    "\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e4c1b-a4cf-480a-8efb-9ba3adba5bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat362-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
