{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204",
   "metadata": {
    "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 1: Gradient Descent with Simple Linear Regression Using For Loop\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    number-sections: true\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b",
   "metadata": {
    "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b"
   },
   "source": [
    "## Tools and Libraries\n",
    "\n",
    "In this quiz, we will make use of:\n",
    "\n",
    "- **NumPy** - A fundamental library for scientific computing in Python, providing support for arrays, mathematical functions, and linear algebra operations\n",
    "- **Matplotlib** - A comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "- **Built-in Python functions** - Including `copy` for creating deep copies of objects and `math` for mathematical operations\n",
    "- **Jupyter notebook magic commands** - Such as `%matplotlib inline` for displaying plots inline and `%autoreload` for automatically reloading modules\n",
    "\n",
    "## Key Python Concepts Used\n",
    "- For loops for iterative computations\n",
    "- NumPy arrays for efficient numerical operations\n",
    "- Function definitions and parameter passing\n",
    "- Gradient descent algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb",
   "metadata": {
    "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153d9384-e918-4346-b704-fe59b219b57c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "153d9384-e918-4346-b704-fe59b219b57c",
    "outputId": "dce00b22-ea03-4f04-c4c1-0bb6a3208d29"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ed48f",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "### Real Estate Price Prediction Challenge\n",
    "\n",
    "Imagine you're a real estate analyst tasked with building a model to predict house prices based on square footage. You have access to a small but valuable dataset from recent sales in your area.\n",
    "\n",
    "**Our Dataset:**\n",
    "We have two recent home sales that will serve as our training data:\n",
    "\n",
    "| House | Size (1000 sqft) | Price (1000s of dollars) | Details |\n",
    "|-------|------------------|--------------------------|---------|\n",
    "| A     | 1.0              | 300                      | 1000 sqft → $300,000 |\n",
    "| B     | 2.0              | 500                      | 2000 sqft → $500,000 |\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this quiz, you will:\n",
    "\n",
    "1. **Implement from scratch** the three core components of gradient descent:\n",
    "   - Cost function computation\n",
    "   - Gradient calculation  \n",
    "   - Parameter optimization loop\n",
    "2. **Apply mathematical concepts** using Python for loops (no vectorization shortcuts!)\n",
    "3. **Visualize the learning process** through cost function plots\n",
    "4. **Make predictions** using your trained model\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Your mission is to find the optimal parameters $(w, b)$ for the linear model:\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ = house size in thousands of square feet\n",
    "- $f_{w,b}(x)$ = predicted price in thousands of dollars\n",
    "- $w$ = slope (price increase per 1000 sqft)\n",
    "- $b$ = y-intercept (base price)\n",
    "\n",
    "**Question to ponder:** Looking at our data, can you estimate what the slope $w$ might be? What does this represent in real-world terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75428a4e-f708-4a61-9a56-288af4599b4a",
   "metadata": {
    "id": "75428a4e-f708-4a61-9a56-288af4599b4a"
   },
   "outputs": [],
   "source": [
    "# Create our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c",
   "metadata": {
    "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c"
   },
   "source": [
    "## Linear Regression Fundamentals\n",
    "\n",
    "In this quiz, you will fit the linear regression parameters $(w,b)$ to your dataset using gradient descent from scratch.\n",
    "\n",
    "### The Linear Model\n",
    "\n",
    "The model function for linear regression maps from input `x` (house size) to output `y` (house price):\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w$ is the **weight** (or slope) - how much the price changes per unit of size\n",
    "- $b$ is the **bias** (or y-intercept) - the base price when size = 0\n",
    "- $x$ is the input feature (house size in 1000s of sqft)\n",
    "- $f_{w,b}(x)$ is the predicted output (house price in 1000s of dollars)\n",
    "\n",
    "### Finding the Best Parameters\n",
    "\n",
    "To train a linear regression model, you need to find the optimal $(w,b)$ parameters:\n",
    "\n",
    "1. **Cost Function Evaluation**: Compare different parameter choices using a cost function $J(w,b)$\n",
    "   - $J(w,b)$ measures how well your model fits the data\n",
    "   - Lower cost = better fit\n",
    "   \n",
    "2. **Optimization Goal**: Find $(w,b)$ that minimizes $J(w,b)$\n",
    "   - The \"best\" parameters are those with the smallest cost\n",
    "   - This gives you the line that best fits your training data\n",
    "\n",
    "3. **Gradient Descent**: Use this iterative algorithm to find optimal parameters\n",
    "   - Start with initial guesses for $w$ and $b$\n",
    "   - Repeatedly adjust parameters in the direction that reduces cost\n",
    "   - Each step moves closer to the optimal values\n",
    "\n",
    "### The Power of Your Model\n",
    "\n",
    "Once trained, your linear regression model becomes a **prediction machine**:\n",
    "\n",
    "- **Input**: Square footage of any house\n",
    "- **Output**: Estimated selling price\n",
    "- **Application**: Help real estate agents, buyers, and sellers make informed decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cde90-255a-4bf0-945a-a904c5008e84",
   "metadata": {
    "id": "817cde90-255a-4bf0-945a-a904c5008e84"
   },
   "source": [
    "## Implement Gradient Descent From Scratch\n",
    "\n",
    "Now comes the exciting part! You will implement the gradient descent algorithm step by step using **for loops only** - no vectorized operations allowed. This approach will help you understand exactly what's happening at each step.\n",
    "\n",
    "### The Three Essential Functions\n",
    "\n",
    "You'll build three interconnected functions that work together:\n",
    "\n",
    "1. **`compute_cost`** - Measures how well your current parameters fit the data\n",
    "2. **`compute_gradient`** - Calculates which direction to adjust your parameters  \n",
    "3. **`gradient_descent`** - Orchestrates the iterative optimization process\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "   ```\n",
    "   Input: Training Data (x, y)\n",
    "      ↓\n",
    "   compute_cost(w, b) → Current fit quality\n",
    "      ↓\n",
    "   compute_gradient(w, b) → Direction to improve\n",
    "      ↓  \n",
    "   gradient_descent() → New improved (w, b)\n",
    "      ↓\n",
    "   Repeat until convergence!\n",
    "   ```\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Why for loops?** While NumPy vectorization is faster, using explicit loops helps you:\n",
    "\n",
    "- Understand each calculation step-by-step\n",
    "- Build intuition for how gradient descent actually works\n",
    "- Appreciate vectorization when you use it later!\n",
    "\n",
    "### Coding Conventions\n",
    "\n",
    "To keep our code readable and mathematically accurate:\n",
    "\n",
    "- **Partial derivatives**: Variables representing $\\frac{\\partial J(w,b)}{\\partial b}$ will be named `dj_db`\n",
    "- **Naming pattern**: `dj_d[parameter]` where `[parameter]` is the variable we're taking the derivative with respect to\n",
    "- **Abbreviation**: \"w.r.t\" = \"With Respect To\" (common mathematical shorthand)\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ → `dj_dw` (derivative of J with respect to w)  \n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ → `dj_db` (derivative of J with respect to b)\n",
    "\n",
    "### Ready to Code?\n",
    "\n",
    "Let's implement each function one by one, building your gradient descent algorithm from the ground up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b",
   "metadata": {
    "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b"
   },
   "source": [
    "### Function 1: Compute_Cost\n",
    "\n",
    "The cost function is your **quality meter** - it tells you how well your current parameters $(w,b)$ fit the training data. Lower cost means better fit!\n",
    "\n",
    "#### Understanding the Cost Function\n",
    "\n",
    "The Mean Squared Error (MSE) cost function measures the average squared difference between predictions and actual values:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "- $f_{w,b}(x^{(i)}) = wx^{(i)} + b$ → Your model's prediction for house $i$\n",
    "- $y^{(i)}$ → Actual selling price for house $i$  \n",
    "- $(f_{w,b}(x^{(i)}) - y^{(i)})^2$ → Squared error for house $i$\n",
    "- $\\frac{1}{2m}$ → Average over all $m$ examples (the $\\frac{1}{2}$ simplifies calculus later!)\n",
    "\n",
    "#### Task 1: compute the cost\n",
    "\n",
    "Complete the `compute_cost` function using for loops to:\n",
    "\n",
    "**Step 1:** For each training example $i$, compute:\n",
    "\n",
    "- **Prediction:** $f_{wb}(x^{(i)}) = wx^{(i)} + b$\n",
    "- **Individual cost:** $cost^{(i)} = (f_{wb}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "**Step 2:** Sum all individual costs and return the total:\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "\n",
    "**Key insight:** You're measuring how \"wrong\" your predictions are on average. The squaring ensures:\n",
    "\n",
    "- All errors are positive (no cancellation between over/under predictions)\n",
    "- Larger errors are penalized more heavily than small errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e",
   "metadata": {
    "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e"
   },
   "outputs": [],
   "source": [
    "# compute_cost\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression using Mean Squared Error.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) - Input features (house sizes in 1000s sqft)\n",
    "        y (ndarray): Shape (m,) - Target values (house prices in 1000s dollars)\n",
    "        w (scalar): Weight parameter (slope of the line)\n",
    "        b (scalar): Bias parameter (y-intercept of the line)\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The cost J(w,b) representing how well the parameters\n",
    "                           fit the training data. Lower cost = better fit.\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # Initialize total cost\n",
    "    total_cost = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Variable to accumulate the sum of squared errors\n",
    "    cost_sum = 0\n",
    "    \n",
    "    # Loop through each training example\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Step 1: Calculate prediction using linear model\n",
    "        f_wb = # YOUR CODE\n",
    "        \n",
    "        # Step 2: Calculate squared error for this example\n",
    "        error = # YOUR CODE\n",
    "        cost = # YOUR CODE\n",
    "        \n",
    "        # Step 3: Add this example's cost to running sum\n",
    "        cost_sum = # YOUR CODE\n",
    "    \n",
    "    # Step 4: Calculate final cost using MSE \n",
    "    total_cost = # YOUR CODE\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad95f76-8295-423b-9337-594bc7fc8c73",
   "metadata": {
    "id": "dad95f76-8295-423b-9337-594bc7fc8c73"
   },
   "source": [
    "#### Test Your Implementation\n",
    "\n",
    "Great job! Now let's verify that your `compute_cost` function works correctly. \n",
    "\n",
    "Run the test code below to check your implementation. The test uses initial parameters `w=2` and `b=1` to see how well they fit our training data.\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "- The function should return a `float` type\n",
    "- The cost value tells you how well these parameters fit the data\n",
    "- Higher cost = worse fit, Lower cost = better fit\n",
    "\n",
    "**For the Canvas quiz:** Record the exact cost value that your function returns - you'll need this number to complete the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01902b",
   "metadata": {},
   "source": [
    "### Function 2: Compute_Gradient\n",
    "\n",
    "The gradient tells you **which direction to move** your parameters to reduce the cost. Think of it as a compass pointing toward better parameter values!\n",
    "\n",
    "#### Understanding Gradients\n",
    "\n",
    "Gradients are partial derivatives that measure how the cost function changes when you slightly adjust each parameter:\n",
    "\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ → How much does cost change if we increase $w$ slightly?\n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ → How much does cost change if we increase $b$ slightly?\n",
    "\n",
    "**The math behind it:** For our cost function $J(w,b) = \\frac{1}{2m} \\sum (f_{w,b}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "The partial derivatives work out to:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b}^{(i)} = (f_{w,b}(x^{(i)}) - y^{(i)})$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w}^{(i)} = (f_{w,b}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "#### Task 2: compute the gradient\n",
    "\n",
    "Complete the `compute_gradient` function using for loops to:\n",
    "\n",
    "**Step 1:** For each training example $i$, compute:\n",
    "\n",
    "- **Prediction:** $f_{wb}(x^{(i)}) = wx^{(i)} + b$\n",
    "- **Error:** $error^{(i)} = f_{wb}(x^{(i)}) - y^{(i)}$ \n",
    "- **Gradient contributions:**\n",
    "  - For $b$: $\\frac{\\partial J}{\\partial b}^{(i)} = error^{(i)}$\n",
    "  - For $w$: $\\frac{\\partial J}{\\partial w}^{(i)} = error^{(i)} \\cdot x^{(i)}$\n",
    "\n",
    "**Step 2:** Average all gradient contributions:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum\\limits_{i=0}^{m-1} \\frac{\\partial J}{\\partial b}^{(i)}$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum\\limits_{i=0}^{m-1} \\frac{\\partial J}{\\partial w}^{(i)}$$\n",
    "\n",
    "**Key insight:** \n",
    "\n",
    "- **Positive gradient** → Increase parameter to reduce cost\n",
    "- **Negative gradient** → Decrease parameter to reduce cost  \n",
    "- **Larger magnitude** → Steeper slope, bigger adjustment needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f",
   "metadata": {
    "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f"
   },
   "outputs": [],
   "source": [
    "# compute_gradient\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (house sizes)\n",
    "      y (ndarray): Shape (m,) Label (house prices)\n",
    "      w, b (scalar): Parameters of the model\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameter w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "     \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # Initialize gradient accumulators\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Loop through each training example\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Step 1: Calculate prediction for example i\n",
    "        f_wb = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 2: Calculate the error (prediction - actual)\n",
    "        error = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 3: Calculate gradient contributions for this example\n",
    "        dj_db_i = # YOUR CODE HERE\n",
    "        \n",
    "        dj_dw_i = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 4: Accumulate the gradients\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i\n",
    "    \n",
    "    # Step 5: Average the gradients over all examples\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254",
   "metadata": {
    "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254"
   },
   "source": [
    "#### Test Your Gradient Function\n",
    "\n",
    "Excellent work! Now let's test your `compute_gradient` function to see if it correctly calculates the gradients.\n",
    "\n",
    "Run the test below to verify your implementation. We'll start with parameters `w=0` and `b=0` to see what gradients your function computes.\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "- The function should return two values: `dj_dw` and `dj_db`\n",
    "- These represent the direction and magnitude to adjust each parameter\n",
    "- The values tell you how to improve your model's fit to the data\n",
    "\n",
    "**For the Canvas quiz:** Record the exact gradient values that your function returns - you'll need these numbers for the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
    "outputId": "41ef6c10-376f-4d7f-a8a6-b7eac22013a2"
   },
   "outputs": [],
   "source": [
    "# Compute and display gradient with w and b initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d647d55",
   "metadata": {},
   "source": [
    "### Function 3: Do Gradient Decent\n",
    "\n",
    "After you implemented `compute_gradient` which calculates $\\frac{\\partial J(w)}{\\partial w}$, $\\frac{\\partial J(w)}{\\partial b}$ , you will next implement the gradient descent for parameters $w, b$ for linear regression.\n",
    "\n",
    "As described in the lecture, the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}   \\; &\n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $w, b$ are both updated simultaniously and where  \n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \n",
    "$$\n",
    "\n",
    "* $m$ is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
    "a.lgorithm.\n",
    "\n",
    "#### Task 3: do gradient descent\n",
    "\n",
    "Please complete the `gradient_descent` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8d754-3045-4492-908b-9f74df6c70fd",
   "metadata": {
    "id": "ddf8d754-3045-4492-908b-9f74df6c70fd"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to optimize parameters w and b\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) - Input features (house sizes)\n",
    "      y (ndarray): Shape (m,) - Target values (house prices)  \n",
    "      w_in, b_in (scalar): Initial parameter values\n",
    "      cost_function: Function to compute cost J(w,b)\n",
    "      gradient_function: Function to compute gradients dJ/dw, dJ/db\n",
    "      alpha (float): Learning rate - how big steps to take\n",
    "      num_iters (int): Number of iterations to run\n",
    "    Returns:\n",
    "      w, b (scalar): Optimized parameter values\n",
    "      J_history (list): Cost at each iteration (for plotting)\n",
    "      w_history (list): Parameter values at each iteration (for plotting)\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # Arrays to store history for plotting learning curves\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    # Initialize parameters (make copies to avoid modifying inputs)\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Main gradient descent loop\n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # Step 1: Compute gradients using your gradient function\n",
    "        dj_dw, dj_db = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 2: Update parameters using gradient descent rule\n",
    "        w = # YOUR CODE HERE\n",
    "        b = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 3: Compute and save cost for this iteration (for plotting)\n",
    "        if i < 100000:  # Prevent memory issues for very long runs\n",
    "            cost = # YOUR CODE HERE \n",
    "            J_history.append(cost)\n",
    "            w_history.append([w, b])\n",
    "        \n",
    "        # Print progress every 10% of iterations\n",
    "        if i % math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return w, b, J_history, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f",
   "metadata": {
    "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f"
   },
   "source": [
    "\n",
    "####  Find Your Optimal Parameters via your gradient descent\n",
    "\n",
    "Now let's run your gradient descent algorithm to discover the optimal values of $w$ and $b$ for our real estate dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
    "outputId": "22154047-5730-4eac-a25a-d887ab7ed8a4"
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w, b, J_hist, p_hist = gradient_descent(x_train ,y_train, initial_w, initial_b,\n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b7112-7a06-481e-8daf-f0852fd885ec",
   "metadata": {
    "id": "867b7112-7a06-481e-8daf-f0852fd885ec"
   },
   "source": [
    "## 📈 Visualizing the Learning Process: Cost Function Evolution\n",
    "\n",
    "### Understanding Your Algorithm's Journey\n",
    "\n",
    "Now comes one of the most exciting parts - watching your gradient descent algorithm learn in real time! The learning curve visualization will show you exactly how your algorithm improved with each iteration.\n",
    "\n",
    "### What You'll Observe in the Plots\n",
    "\n",
    "**Two-Panel Visualization:**\n",
    "\n",
    "1. **Left Panel (Early Learning):** Shows the first 100 iterations\n",
    "   \n",
    "   - **Expect:** Sharp, rapid decrease in cost\n",
    "   - **Why:** Initial parameter values are far from optimal, so big improvements happen quickly\n",
    "   - **Learning:** This demonstrates the power of gradient descent's initial convergence\n",
    "\n",
    "2. **Right Panel (Fine-Tuning):** Shows iterations 1000+ to the end\n",
    "   \n",
    "   - **Expect:** Gradual, steady decrease approaching a minimum\n",
    "   - **Why:** Algorithm is fine-tuning parameters near the optimal solution\n",
    "   - **Learning:** Shows how gradient descent achieves precision through patience\n",
    "\n",
    "### Key Learning Insights\n",
    "\n",
    "**Cost Decrease Patterns:**\n",
    "\n",
    "- **Steep drop** → Your algorithm is making big improvements (far from optimum)\n",
    "- **Gradual decline** → Fine-tuning phase (approaching optimum)  \n",
    "- **Plateau** → Convergence achieved (found the minimum!)\n",
    "\n",
    "**What This Tells You:**\n",
    "\n",
    "- **Algorithm Health:** Consistently decreasing cost = healthy learning\n",
    "- **Learning Rate Quality:** Smooth curves = good learning rate choice\n",
    "- **Convergence Status:** Flattening curve = approaching optimal solution\n",
    "\n",
    "**For the Canvas Quiz:** Pay attention to the final cost value and how many iterations it took to converge - these observations may be part of your quiz questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41184459-21ed-4bbb-a371-24409929c056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "41184459-21ed-4bbb-a371-24409929c056",
    "outputId": "993fe0ea-76e5-4126-851b-0571db16486c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7",
   "metadata": {
    "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7"
   },
   "source": [
    "##  Making Predictions with Your Trained Model\n",
    "\n",
    "### Task 4: Real Estate Price Prediction Challenge\n",
    "\n",
    "**The Moment of Truth!** \n",
    "Now that you have trained your model and found optimal parameters $w$ and $b$, it's time to put your **prediction machine** to work! This is where all your hard work pays off.\n",
    "\n",
    "### Understanding Your Prediction Tool\n",
    "\n",
    "**The Prediction Formula:**\n",
    "Your linear regression model makes predictions using the simple but powerful equation:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "**Breaking Down Each Component:**\n",
    "\n",
    "- **$w$ (slope)** = Price increase per 1000 sqft → *\"How much more does each additional 1000 sqft cost?\"*\n",
    "- **$b$ (y-intercept)** = Base price when size = 0 → *\"What's the starting price before considering square footage?\"*\n",
    "- **$x^{(i)}$ (input)** = House size in thousands of sqft → *\"The feature we're using to predict\"*\n",
    "- **$f_{w,b}(x^{(i)})$ (output)** = Predicted price in thousands of dollars → *\"Our model's best guess\"*\n",
    "\n",
    "###  Your Real Estate Consultation Challenge\n",
    "\n",
    "**The Scenario:**\n",
    "\n",
    "A potential buyer walks into your real estate office and asks:\n",
    "\n",
    "> *\"I'm interested in buying a house with exactly **1200 square feet**. Based on recent market data, what should I expect to pay?\"*\n",
    "\n",
    "**Your Mission:**\n",
    "\n",
    "1. **Use your trained parameters** $(w, b)$ found by gradient descent\n",
    "2. **Convert units properly** (1200 sqft → thousands of sqft)\n",
    "3. **Calculate the prediction** using your linear model\n",
    "4. **Provide a professional estimate** to your client\n",
    "\n",
    "\n",
    "**For the Canvas Quiz:** 📝 Record your predicted price carefully - you'll need this exact number for the quiz questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SF2ScjY1RYJ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF2ScjY1RYJ8",
    "outputId": "6c2211ce-6bba-4eae-c560-ad7e3203d795"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# House size: 1200 sqft\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccd1ed",
   "metadata": {},
   "source": [
    "##  Bonus Exploration: Learning Rate Sensitivity Analysis\n",
    "\n",
    "**Experiment Goal:** Run the following code cells and investigate how different learning rates affect gradient descent convergence and performance\n",
    "\n",
    "###  The Learning Rate Experiment\n",
    "\n",
    "Learning rate (α) is one of the most critical hyperparameters in machine learning. This experiment will demonstrate how this single parameter can dramatically affect your algorithm's behavior, convergence speed, and final performance.\n",
    "\n",
    "###  Experimental Design\n",
    "\n",
    "**Test Parameters:**\n",
    "- Learning rates: 0.001, 0.01, 0.1, 0.5\n",
    "- Iterations: 1000 (sufficient to observe different behaviors)\n",
    "- Initial conditions: w=0, b=0 (same for all tests)\n",
    "- Metric: Cost function evolution over time\n",
    "\n",
    "**Hypothesis:** Different learning rates will show distinct convergence patterns - some too slow, some too fast, and hopefully one \"just right\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different learning rates to compare convergence behavior\n",
    "# This helps understand how learning rate affects training dynamics\n",
    "\n",
    "# Define different learning rates to test\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "iterations = 1000\n",
    "\n",
    "# Store results for each learning rate\n",
    "results = {}\n",
    "\n",
    "print(\"Testing different learning rates:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    print(f\"\\nTesting learning rate: {alpha}\")\n",
    "    \n",
    "    # Reset initial parameters for fair comparison\n",
    "    initial_w = 0.0\n",
    "    initial_b = 0.0\n",
    "    \n",
    "    # Run gradient descent with current learning rate\n",
    "    w_final, b_final, J_history, p_history = gradient_descent(\n",
    "        x_train, y_train, initial_w, initial_b, \n",
    "        compute_cost, compute_gradient, alpha, iterations\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[alpha] = {\n",
    "        'w': w_final,\n",
    "        'b': b_final,\n",
    "        'cost_history': J_history,\n",
    "        'final_cost': J_history[-1] if J_history else float('inf')\n",
    "    }\n",
    "    \n",
    "    print(f\"Final parameters: w={w_final:.6f}, b={b_final:.6f}\")\n",
    "    print(f\"Final cost: {J_history[-1]:.8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Learning Rate Comparison Summary:\")\n",
    "for alpha in learning_rates:\n",
    "    result = results[alpha]\n",
    "    print(f\"α = {alpha:5.3f}: Final Cost = {result['final_cost']:.8f}, w = {result['w']:.6f}, b = {result['b']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3183e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of learning curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All learning curves together (full range)\n",
    "ax1.set_title('Learning Curves: All Learning Rates (Full Range)', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    ax1.plot(cost_history, color=colors[i], label=f'α = {alpha}', linewidth=2)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Cost J(w,b)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Early iterations (first 100) - shows initial convergence behavior\n",
    "ax2.set_title('Early Learning Phase (First 100 Iterations)', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    early_history = cost_history[:min(100, len(cost_history))]\n",
    "    ax2.plot(early_history, color=colors[i], label=f'α = {alpha}', linewidth=2)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Cost J(w,b)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Log scale view - better for comparing convergence rates\n",
    "ax3.set_title('Learning Curves (Log Scale) - Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    # Add small epsilon to avoid log(0) issues\n",
    "    log_costs = [max(cost, 1e-10) for cost in cost_history]\n",
    "    ax3.semilogy(log_costs, color=colors[i], label=f'α = {alpha}', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Cost J(w,b) (Log Scale)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final cost comparison (bar chart)\n",
    "ax4.set_title('Final Cost Comparison After 1000 Iterations', fontsize=14, fontweight='bold')\n",
    "final_costs = [results[alpha]['final_cost'] for alpha in learning_rates]\n",
    "bars = ax4.bar(range(len(learning_rates)), final_costs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Learning Rate')\n",
    "ax4.set_ylabel('Final Cost')\n",
    "ax4.set_xticks(range(len(learning_rates)))\n",
    "ax4.set_xticklabels([f'α = {alpha}' for alpha in learning_rates])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, cost) in enumerate(zip(bars, final_costs)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(final_costs)*0.01, \n",
    "             f'{cost:.6f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis and insights\n",
    "print(\"\\n🔍 LEARNING RATE ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performing learning rate\n",
    "best_alpha = min(learning_rates, key=lambda x: results[x]['final_cost'])\n",
    "worst_alpha = max(learning_rates, key=lambda x: results[x]['final_cost'])\n",
    "\n",
    "print(f\"🏆 BEST Learning Rate: α = {best_alpha}\")\n",
    "print(f\"   Final Cost: {results[best_alpha]['final_cost']:.8f}\")\n",
    "print(f\"   Parameters: w = {results[best_alpha]['w']:.6f}, b = {results[best_alpha]['b']:.6f}\")\n",
    "\n",
    "print(f\"\\n❌ WORST Learning Rate: α = {worst_alpha}\")\n",
    "print(f\"   Final Cost: {results[worst_alpha]['final_cost']:.8f}\")\n",
    "print(f\"   Parameters: w = {results[worst_alpha]['w']:.6f}, b = {results[worst_alpha]['b']:.6f}\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"   • Lower learning rates → More stable but slower convergence\")\n",
    "print(\"   • Higher learning rates → Faster initial progress but may overshoot\")\n",
    "print(\"   • Optimal learning rate balances speed and stability\")\n",
    "print(\"   • Learning rate choice significantly impacts training efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c009e",
   "metadata": {},
   "source": [
    "###  Your Insights and Reflection\n",
    "\n",
    "**Analyze the results above and answer these questions:**\n",
    "\n",
    "####  Observation Questions:\n",
    "\n",
    "1. **Which learning rate achieved the lowest final cost? Why do you think this happened?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "   \n",
    "2. **Compare the convergence speed between α=0.001 and α=0.1. What trade-offs do you observe?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "   \n",
    "3. **Looking at the early convergence plot (first 100 iterations), which learning rate made the most dramatic initial progress? Was this rate also the best overall performer?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "####  Critical Thinking Questions:\n",
    "\n",
    "4. **If you had to choose ONE learning rate for a production machine learning system, which would you choose and why?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "5. **What would you expect to happen if we tested an even larger learning rate like α=1.0? Explain your reasoning.**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "6. **How might the optimal learning rate change if we had:**\n",
    "   - A much larger dataset (1000s of examples)?\n",
    "   - A more complex cost function landscape?\n",
    "   \n",
    "   *Your answers:*\n",
    "   \n",
    "\n",
    "####  Real-World Application:\n",
    "\n",
    "7. **You're a data scientist at a real estate company. Based on this experiment, what would you tell your manager about the importance of learning rate tuning?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "Write 3-5 bullet points summarizing the most important lessons from this learning rate sensitivity analysis:\n",
    "\n",
    "- *Your takeaway 1:*\n",
    "- *Your takeaway 2:*  \n",
    "- *Your takeaway 3:*\n",
    "- *Your takeaway 4:*\n",
    "- *Your takeaway 5:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344729aa-f0e6-4910-b08a-c0c667c964fc",
   "metadata": {
    "id": "344729aa-f0e6-4910-b08a-c0c667c964fc"
   },
   "source": [
    "## 🏆 Congratulations! You've Mastered Gradient Descent!\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "In this quiz, you successfully:\n",
    "\n",
    "**🔬 Built from Scratch:**\n",
    "\n",
    "- Cost function using Mean Squared Error\n",
    "- Gradient calculations with partial derivatives  \n",
    "- Complete gradient descent optimization algorithm\n",
    "\n",
    "**🏠 Solved Real Problems:**\n",
    "\n",
    "- Predicted house prices using machine learning\n",
    "- Trained a model on actual data\n",
    "- Made business-ready predictions\n",
    "\n",
    "**📊 Understood the Process:**\n",
    "\n",
    "- Visualized algorithm learning through cost plots\n",
    "- Observed parameter convergence in real-time\n",
    "- Connected math to practical applications\n",
    "\n",
    "\n",
    "### Next Steps in Your Learning Journey\n",
    "\n",
    "You're now ready for:\n",
    "\n",
    "- **Multi-variable regression** and beyond!\n",
    "- **Vectorized implementations** for better performance\n",
    "- **Advanced algorithms** (Adam, RMSprop, etc.)\n",
    "\n",
    "\n",
    "**Key Insight:** Every advanced ML algorithm builds on these fundamentals you've mastered!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538",
   "metadata": {
    "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538"
   },
   "source": [
    "## Reference\n",
    "\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f328376-71d5-46af-883d-06d9feba7383",
   "metadata": {
    "id": "0f328376-71d5-46af-883d-06d9feba7383"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stat362-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
